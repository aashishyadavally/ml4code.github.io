[["feng2020codebert", "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"], ["naik2022probing", "Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis"], ["guo2020graphcodebert", "GraphCodeBERT: Pre-training Code Representations with Data Flow"], ["wan2022what", "What Do They Capture? -- A Structural Analysis of Pre-Trained Language Models for Source Code"]]